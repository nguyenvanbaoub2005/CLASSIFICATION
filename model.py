# model.py
"""
File ch·ª©a ki·∫øn tr√∫c model CNN cho ph√¢n lo·∫°i r√°c th·∫£i
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from config import MODEL_CONFIG


def create_waste_classifier_model(input_shape=None, num_classes=None):
    """
    T·∫°o model CNN ƒë·ªÉ ph√¢n lo·∫°i r√°c th·∫£i
    
    Args:
        input_shape: K√≠ch th∆∞·ªõc input (height, width, channels)
        num_classes: S·ªë l∆∞·ª£ng classes c·∫ßn ph√¢n lo·∫°i
    
    Returns:
        model: Keras model ƒë√£ compile
    """
    if input_shape is None:
        input_shape = MODEL_CONFIG['input_shape']
    if num_classes is None:
        num_classes = MODEL_CONFIG['num_classes']
    
    model = keras.Sequential([
        # Block 1
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),
        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.BatchNormalization(),
        layers.Dropout(0.25),
        
        # Block 2
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.BatchNormalization(),
        layers.Dropout(0.25),
        
        # Block 3
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.BatchNormalization(),
        layers.Dropout(0.25),
        
        # Block 4
        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.BatchNormalization(),
        layers.Dropout(0.25),
        
        # Flatten v√† Dense layers
        layers.Flatten(),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        layers.Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=MODEL_CONFIG['learning_rate']),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model


def create_transfer_learning_model(base_model_name='MobileNetV2', input_shape=None, num_classes=None):
    """
    T·∫°o model s·ª≠ d·ª•ng transfer learning
    
    Args:
        base_model_name: T√™n model g·ªëc ('MobileNetV2', 'VGG16', 'ResNet50')
        input_shape: K√≠ch th∆∞·ªõc input
        num_classes: S·ªë l∆∞·ª£ng classes
    
    Returns:
        model: Keras model v·ªõi transfer learning
    """
    if input_shape is None:
        input_shape = MODEL_CONFIG['input_shape']
    if num_classes is None:
        num_classes = MODEL_CONFIG['num_classes']
    
    # Ch·ªçn base model
    if base_model_name == 'MobileNetV2':
        base_model = keras.applications.MobileNetV2(
            input_shape=input_shape,
            include_top=False,
            weights='imagenet'
        )
    elif base_model_name == 'VGG16':
        base_model = keras.applications.VGG16(
            input_shape=input_shape,
            include_top=False,
            weights='imagenet'
        )
    elif base_model_name == 'ResNet50':
        base_model = keras.applications.ResNet50(
            input_shape=input_shape,
            include_top=False,
            weights='imagenet'
        )
    else:
        raise ValueError(f"Kh√¥ng h·ªó tr·ª£ base model: {base_model_name}")
    
    # ƒê√≥ng bƒÉng c√°c layer c·ªßa base model
    base_model.trainable = False
    
    # Th√™m custom layers
    model = keras.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=MODEL_CONFIG['learning_rate']),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model


def get_model_summary(model):
    """In th√¥ng tin t√≥m t·∫Øt v·ªÅ model"""
    print("\n" + "="*70)
    print("üìä TH√îNG TIN MODEL")
    print("="*70)
    model.summary()
    print("="*70 + "\n")
    
    total_params = model.count_params()
    print(f"T·ªïng s·ªë parameters: {total_params:,}")
    return total_params