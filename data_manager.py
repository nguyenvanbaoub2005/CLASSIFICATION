# data_manager.py
"""
Qu·∫£n l√Ω v√† t·ªï ch·ª©c d·ªØ li·ªáu ƒë√£ scan ƒë·ªÉ training
"""

import os
import json
import shutil
from datetime import datetime
from config import CLASSES
import random


class DataManager:
    """Class qu·∫£n l√Ω d·ªØ li·ªáu training"""
    
    def __init__(self, scanned_dir="scanned_data", dataset_dir="dataset"):
        self.scanned_dir = scanned_dir
        self.dataset_dir = dataset_dir
        self.train_ratio = 0.8  # 80% train, 20% validation
        
    def get_scanned_stats(self):
        """Th·ªëng k√™ d·ªØ li·ªáu ƒë√£ scan"""
        stats = {}
        total = 0
        high_conf_count = 0
        
        for cls in CLASSES:
            cls_dir = os.path.join(self.scanned_dir, cls)
            if os.path.exists(cls_dir):
                images = [f for f in os.listdir(cls_dir) if f.endswith('.jpg')]
                stats[cls] = {
                    'count': len(images),
                    'high_confidence': 0,
                    'low_confidence': 0
                }
                
                # ƒê·∫øm theo confidence
                for img in images:
                    json_path = os.path.join(cls_dir, img.replace('.jpg', '.json'))
                    if os.path.exists(json_path):
                        with open(json_path, 'r') as f:
                            data = json.load(f)
                            if data['confidence'] >= 80:
                                stats[cls]['high_confidence'] += 1
                                high_conf_count += 1
                            else:
                                stats[cls]['low_confidence'] += 1
                
                total += stats[cls]['count']
            else:
                stats[cls] = {'count': 0, 'high_confidence': 0, 'low_confidence': 0}
        
        return {
            'total': total,
            'high_confidence': high_conf_count,
            'by_class': stats
        }
    
    def prepare_training_data(self, min_confidence=80, use_all=False):
        """
        Chu·∫©n b·ªã d·ªØ li·ªáu t·ª´ scanned_data v√†o dataset cho training
        
        Args:
            min_confidence: ƒê·ªô tin c·∫≠y t·ªëi thi·ªÉu
            use_all: S·ª≠ d·ª•ng t·∫•t c·∫£ d·ªØ li·ªáu (kh√¥ng ph√¢n chia train/val)
        
        Returns:
            dict: Th√¥ng tin v·ªÅ d·ªØ li·ªáu ƒë√£ chu·∫©n b·ªã
        """
        print("\n" + "="*70)
        print("üì¶ CHU·∫®N B·ªä D·ªÆ LI·ªÜU TRAINING")
        print("="*70)
        
        # T·∫°o th∆∞ m·ª•c dataset
        train_dir = os.path.join(self.dataset_dir, 'train')
        val_dir = os.path.join(self.dataset_dir, 'validation')
        
        for cls in CLASSES:
            os.makedirs(os.path.join(train_dir, cls), exist_ok=True)
            os.makedirs(os.path.join(val_dir, cls), exist_ok=True)
        
        stats = {'train': {}, 'val': {}}
        
        for cls in CLASSES:
            cls_scanned = os.path.join(self.scanned_dir, cls)
            cls_train = os.path.join(train_dir, cls)
            cls_val = os.path.join(val_dir, cls)
            
            if not os.path.exists(cls_scanned):
                continue
            
            # L·∫•y t·∫•t c·∫£ ·∫£nh ƒë·ªß ƒëi·ªÅu ki·ªán
            valid_images = []
            for img_file in os.listdir(cls_scanned):
                if not img_file.endswith('.jpg'):
                    continue
                
                json_path = os.path.join(cls_scanned, img_file.replace('.jpg', '.json'))
                if os.path.exists(json_path):
                    with open(json_path, 'r') as f:
                        data = json.load(f)
                        if data['confidence'] >= min_confidence:
                            valid_images.append(img_file)
            
            if not valid_images:
                print(f"‚ö†Ô∏è  {cls}: Kh√¥ng c√≥ ·∫£nh n√†o ƒë·ªß ƒëi·ªÅu ki·ªán")
                continue
            
            # Shuffle
            random.shuffle(valid_images)
            
            # Ph√¢n chia train/val
            split_idx = int(len(valid_images) * self.train_ratio)
            train_images = valid_images[:split_idx]
            val_images = valid_images[split_idx:]
            
            # Copy files
            for img in train_images:
                src = os.path.join(cls_scanned, img)
                dst = os.path.join(cls_train, img)
                shutil.copy2(src, dst)
            
            for img in val_images:
                src = os.path.join(cls_scanned, img)
                dst = os.path.join(cls_val, img)
                shutil.copy2(src, dst)
            
            stats['train'][cls] = len(train_images)
            stats['val'][cls] = len(val_images)
            
            print(f"‚úì {cls:12s}: {len(train_images)} train, {len(val_images)} val")
        
        print("="*70)
        print(f"‚úÖ Ho√†n t·∫•t! Dataset s·∫µn s√†ng t·∫°i: {self.dataset_dir}")
        
        return stats
    
    def merge_with_existing_dataset(self, existing_train_dir, existing_val_dir):
        """
        Merge d·ªØ li·ªáu m·ªõi v·ªõi dataset c≈©
        
        Args:
            existing_train_dir: Th∆∞ m·ª•c train hi·ªán c√≥
            existing_val_dir: Th∆∞ m·ª•c validation hi·ªán c√≥
        """
        print("\nüîÄ MERGE D·ªÆ LI·ªÜU M·ªöI V·ªöI DATASET C≈®...")
        
        for cls in CLASSES:
            # Train
            src_train = os.path.join(self.dataset_dir, 'train', cls)
            dst_train = os.path.join(existing_train_dir, cls)
            
            if os.path.exists(src_train):
                os.makedirs(dst_train, exist_ok=True)
                for img in os.listdir(src_train):
                    if img.endswith('.jpg'):
                        shutil.copy2(
                            os.path.join(src_train, img),
                            os.path.join(dst_train, img)
                        )
            
            # Validation
            src_val = os.path.join(self.dataset_dir, 'validation', cls)
            dst_val = os.path.join(existing_val_dir, cls)
            
            if os.path.exists(src_val):
                os.makedirs(dst_val, exist_ok=True)
                for img in os.listdir(src_val):
                    if img.endswith('.jpg'):
                        shutil.copy2(
                            os.path.join(src_val, img),
                            os.path.join(dst_val, img)
                        )
        
        print("‚úÖ ƒê√£ merge xong!")
    
    def export_high_quality_data(self, output_dir, min_confidence=90):
        """
        Export d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng cao (confidence >= 90%)
        
        Args:
            output_dir: Th∆∞ m·ª•c output
            min_confidence: ƒê·ªô tin c·∫≠y t·ªëi thi·ªÉu
        """
        print(f"\nüì§ EXPORT D·ªÆ LI·ªÜU CH·∫§T L∆Ø·ª¢NG CAO (>={min_confidence}%)...")
        
        for cls in CLASSES:
            cls_dir = os.path.join(self.scanned_dir, cls)
            output_cls_dir = os.path.join(output_dir, cls)
            os.makedirs(output_cls_dir, exist_ok=True)
            
            if not os.path.exists(cls_dir):
                continue
            
            count = 0
            for img_file in os.listdir(cls_dir):
                if not img_file.endswith('.jpg'):
                    continue
                
                json_path = os.path.join(cls_dir, img_file.replace('.jpg', '.json'))
                if os.path.exists(json_path):
                    with open(json_path, 'r') as f:
                        data = json.load(f)
                        if data['confidence'] >= min_confidence:
                            shutil.copy2(
                                os.path.join(cls_dir, img_file),
                                os.path.join(output_cls_dir, img_file)
                            )
                            count += 1
            
            print(f"‚úì {cls:12s}: {count} ·∫£nh")
        
        print(f"‚úÖ ƒê√£ export v√†o: {output_dir}")
    
    def clean_low_quality_data(self, max_confidence=60):
        """
        X√≥a d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng th·∫•p
        
        Args:
            max_confidence: X√≥a ·∫£nh c√≥ confidence <= gi√° tr·ªã n√†y
        """
        print(f"\nüóëÔ∏è  X√ìA D·ªÆ LI·ªÜU CH·∫§T L∆Ø·ª¢NG TH·∫§P (<={max_confidence}%)...")
        
        total_removed = 0
        
        for cls in CLASSES:
            cls_dir = os.path.join(self.scanned_dir, cls)
            if not os.path.exists(cls_dir):
                continue
            
            removed = 0
            for img_file in os.listdir(cls_dir):
                if not img_file.endswith('.jpg'):
                    continue
                
                json_path = os.path.join(cls_dir, img_file.replace('.jpg', '.json'))
                if os.path.exists(json_path):
                    with open(json_path, 'r') as f:
                        data = json.load(f)
                        if data['confidence'] <= max_confidence:
                            os.remove(os.path.join(cls_dir, img_file))
                            os.remove(json_path)
                            removed += 1
            
            if removed > 0:
                print(f"‚úì {cls:12s}: ƒê√£ x√≥a {removed} ·∫£nh")
                total_removed += removed
        
        print(f"‚úÖ T·ªïng ƒë√£ x√≥a: {total_removed} ·∫£nh")
    
    def generate_report(self):
        """T·∫°o b√°o c√°o chi ti·∫øt"""
        print("\n" + "="*70)
        print("üìä B√ÅO C√ÅO D·ªÆ LI·ªÜU")
        print("="*70)
        
        stats = self.get_scanned_stats()
        
        print(f"\nT·ªïng s·ªë m·∫´u: {stats['total']}")
        print(f"Ch·∫•t l∆∞·ª£ng cao (‚â•80%): {stats['high_confidence']}")
        print(f"T·ª∑ l·ªá ch·∫•t l∆∞·ª£ng: {stats['high_confidence']/stats['total']*100:.1f}%")
        
        print("\n" + "-"*70)
        print(f"{'Class':12s} | {'T·ªïng':>6s} | {'Cao':>6s} | {'Th·∫•p':>6s} | {'% Cao':>8s}")
        print("-"*70)
        
        for cls in CLASSES:
            data = stats['by_class'][cls]
            total = data['count']
            high = data['high_confidence']
            low = data['low_confidence']
            pct = (high/total*100) if total > 0 else 0
            
            print(f"{cls:12s} | {total:6d} | {high:6d} | {low:6d} | {pct:7.1f}%")
        
        print("="*70)
        
        # Khuy·∫øn ngh·ªã
        print("\nüí° KHUY·∫æN NGH·ªä:")
        min_samples = min(stats['by_class'][cls]['count'] for cls in CLASSES)
        
        if stats['total'] < 500:
            print("‚ö†Ô∏è  C·∫ßn th√™m d·ªØ li·ªáu (t·ªëi thi·ªÉu 500 m·∫´u)")
        elif min_samples < 50:
            print("‚ö†Ô∏è  M·ªôt s·ªë class thi·∫øu d·ªØ li·ªáu (c·∫ßn √≠t nh·∫•t 50 m·∫´u/class)")
        elif stats['high_confidence'] / stats['total'] < 0.7:
            print("‚ö†Ô∏è  Nhi·ªÅu m·∫´u ch·∫•t l∆∞·ª£ng th·∫•p - n√™n review l·∫°i")
        else:
            print("‚úÖ D·ªØ li·ªáu ƒë·ªß ƒëi·ªÅu ki·ªán ƒë·ªÉ training!")


def main():
    """Main function"""
    manager = DataManager()
    
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë              üìä QU·∫¢N L√ù D·ªÆ LI·ªÜU TRAINING                  ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    while True:
        print("\n" + "="*70)
        print("MENU:")
        print("1. Xem th·ªëng k√™ d·ªØ li·ªáu")
        print("2. Chu·∫©n b·ªã d·ªØ li·ªáu cho training")
        print("3. Export d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng cao")
        print("4. X√≥a d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng th·∫•p")
        print("5. T·∫°o b√°o c√°o chi ti·∫øt")
        print("0. Tho√°t")
        print("="*70)
        
        choice = input("\nNh·∫≠p l·ª±a ch·ªçn: ").strip()
        
        if choice == '1':
            stats = manager.get_scanned_stats()
            print(f"\nT·ªïng: {stats['total']} m·∫´u")
            print(f"Ch·∫•t l∆∞·ª£ng cao: {stats['high_confidence']}")
            for cls, data in stats['by_class'].items():
                print(f"  {cls:12s}: {data['count']:4d} (High: {data['high_confidence']}, Low: {data['low_confidence']})")
        
        elif choice == '2':
            min_conf = input("ƒê·ªô tin c·∫≠y t·ªëi thi·ªÉu (Enter=80): ").strip()
            min_conf = int(min_conf) if min_conf else 80
            manager.prepare_training_data(min_confidence=min_conf)
        
        elif choice == '3':
            output = input("Th∆∞ m·ª•c output (Enter=high_quality_data): ").strip()
            output = output if output else "high_quality_data"
            min_conf = input("ƒê·ªô tin c·∫≠y t·ªëi thi·ªÉu (Enter=90): ").strip()
            min_conf = int(min_conf) if min_conf else 90
            manager.export_high_quality_data(output, min_conf)
        
        elif choice == '4':
            confirm = input("‚ö†Ô∏è  B·∫°n c√≥ ch·∫Øc mu·ªën x√≥a? (yes/no): ").strip().lower()
            if confirm == 'yes':
                max_conf = input("X√≥a ·∫£nh c√≥ confidence <= (Enter=60): ").strip()
                max_conf = int(max_conf) if max_conf else 60
                manager.clean_low_quality_data(max_conf)
        
        elif choice == '5':
            manager.generate_report()
        
        elif choice == '0':
            break
        
        input("\nNh·∫•n Enter ƒë·ªÉ ti·∫øp t·ª•c...")


if __name__ == "__main__":
    main()