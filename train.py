# train.py
"""
File huáº¥n luyá»‡n model phÃ¢n loáº¡i rÃ¡c tháº£i
"""

import os
import matplotlib.pyplot as plt
from tensorflow import keras
from model import create_waste_classifier_model, create_transfer_learning_model, get_model_summary
from config import MODEL_CONFIG, PATHS, AUGMENTATION_CONFIG


def create_data_generators(train_dir, val_dir, batch_size=None):
    """
    Táº¡o data generators cho training vÃ  validation
    
    Args:
        train_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c training data
        val_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c validation data
        batch_size: KÃ­ch thÆ°á»›c batch
    
    Returns:
        train_generator, val_generator
    """
    if batch_size is None:
        batch_size = MODEL_CONFIG['batch_size']
    
    # Data Augmentation cho training
    train_datagen = keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255,
        rotation_range=AUGMENTATION_CONFIG['rotation_range'],
        width_shift_range=AUGMENTATION_CONFIG['width_shift_range'],
        height_shift_range=AUGMENTATION_CONFIG['height_shift_range'],
        horizontal_flip=AUGMENTATION_CONFIG['horizontal_flip'],
        zoom_range=AUGMENTATION_CONFIG['zoom_range'],
        shear_range=AUGMENTATION_CONFIG['shear_range'],
        fill_mode=AUGMENTATION_CONFIG['fill_mode']
    )
    
    # Chá»‰ rescale cho validation
    val_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)
    
    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=MODEL_CONFIG['input_shape'][:2],
        batch_size=batch_size,
        class_mode='categorical',
        shuffle=True
    )
    
    val_generator = val_datagen.flow_from_directory(
        val_dir,
        target_size=MODEL_CONFIG['input_shape'][:2],
        batch_size=batch_size,
        class_mode='categorical',
        shuffle=False
    )
    
    return train_generator, val_generator


def get_callbacks():
    """Táº¡o callbacks cho training"""
    callbacks = [
        # LÆ°u model tá»‘t nháº¥t
        keras.callbacks.ModelCheckpoint(
            PATHS['best_model'],
            save_best_only=True,
            monitor='val_accuracy',
            mode='max',
            verbose=1
        ),
        
        # Early stopping
        keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),
        
        # Giáº£m learning rate khi plateau
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-7,
            verbose=1
        ),
        
        # TensorBoard logging
        keras.callbacks.TensorBoard(
            log_dir='./logs',
            histogram_freq=1,
            write_graph=True
        )
    ]
    
    return callbacks


def train_model(train_dir, val_dir, epochs=None, batch_size=None, use_transfer_learning=False):
    """
    Huáº¥n luyá»‡n model
    
    Args:
        train_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c training
        val_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c validation
        epochs: Sá»‘ epochs
        batch_size: KÃ­ch thÆ°á»›c batch
        use_transfer_learning: Sá»­ dá»¥ng transfer learning hay khÃ´ng
    
    Returns:
        model, history
    """
    if epochs is None:
        epochs = MODEL_CONFIG['epochs']
    if batch_size is None:
        batch_size = MODEL_CONFIG['batch_size']
    
    print("\nğŸš€ Báº®T Äáº¦U HUáº¤N LUYá»†N MODEL")
    print("="*70)
    
    # Táº¡o data generators
    print("\nğŸ“ Äang táº£i dá»¯ liá»‡u...")
    train_generator, val_generator = create_data_generators(train_dir, val_dir, batch_size)
    
    print(f"\nâœ“ Sá»‘ lÆ°á»£ng áº£nh training: {train_generator.samples}")
    print(f"âœ“ Sá»‘ lÆ°á»£ng áº£nh validation: {val_generator.samples}")
    print(f"âœ“ Sá»‘ classes: {len(train_generator.class_indices)}")
    print(f"âœ“ Classes: {list(train_generator.class_indices.keys())}")
    
    # Táº¡o model
    print("\nğŸ—ï¸  Äang xÃ¢y dá»±ng model...")
    if use_transfer_learning:
        print("   Sá»­ dá»¥ng Transfer Learning (MobileNetV2)")
        model = create_transfer_learning_model('MobileNetV2')
    else:
        print("   Sá»­ dá»¥ng CNN tá»« Ä‘áº§u")
        model = create_waste_classifier_model()
    
    get_model_summary(model)
    
    # Callbacks
    callbacks = get_callbacks()
    
    # Training
    print("\nğŸ¯ Báº¯t Ä‘áº§u training...")
    print(f"   Epochs: {epochs}")
    print(f"   Batch size: {batch_size}")
    print(f"   Steps per epoch: {train_generator.samples // batch_size}")
    print("="*70 + "\n")
    
    history = model.fit(
        train_generator,
        epochs=epochs,
        validation_data=val_generator,
        callbacks=callbacks,
        verbose=1
    )
    
    # LÆ°u model cuá»‘i cÃ¹ng
    model.save(PATHS['model_save'])
    print(f"\nâœ… Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {PATHS['model_save']}")
    
    return model, history


def plot_training_history(history):
    """
    Váº½ biá»ƒu Ä‘á»“ quÃ¡ trÃ¬nh training
    
    Args:
        history: History object tá»« model.fit()
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Accuracy
    axes[0, 0].plot(history.history['accuracy'], label='Train', linewidth=2)
    axes[0, 0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)
    axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # Loss
    axes[0, 1].plot(history.history['loss'], label='Train', linewidth=2)
    axes[0, 1].plot(history.history['val_loss'], label='Validation', linewidth=2)
    axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Accuracy comparison
    final_train_acc = history.history['accuracy'][-1]
    final_val_acc = history.history['val_accuracy'][-1]
    axes[1, 0].bar(['Training', 'Validation'], [final_train_acc, final_val_acc], 
                   color=['#2ecc71', '#3498db'])
    axes[1, 0].set_title('Final Accuracy Comparison', fontsize=14, fontweight='bold')
    axes[1, 0].set_ylabel('Accuracy')
    axes[1, 0].set_ylim([0, 1])
    for i, v in enumerate([final_train_acc, final_val_acc]):
        axes[1, 0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')
    
    # Loss comparison
    final_train_loss = history.history['loss'][-1]
    final_val_loss = history.history['val_loss'][-1]
    axes[1, 1].bar(['Training', 'Validation'], [final_train_loss, final_val_loss],
                   color=['#e74c3c', '#f39c12'])
    axes[1, 1].set_title('Final Loss Comparison', fontsize=14, fontweight='bold')
    axes[1, 1].set_ylabel('Loss')
    for i, v in enumerate([final_train_loss, final_val_loss]):
        axes[1, 1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(PATHS['training_plot'], dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š Biá»ƒu Ä‘á»“ training Ä‘Ã£ lÆ°u táº¡i: {PATHS['training_plot']}")
    plt.show()


def main():
    """Main function cho training"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘           ğŸ“ HUáº¤N LUYá»†N MODEL PHÃ‚N LOáº I RÃC THáº¢I         â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Nháº­p thÃ´ng tin
    train_dir = input("ğŸ“ ÄÆ°á»ng dáº«n thÆ° má»¥c training: ").strip()
    val_dir = input("ğŸ“ ÄÆ°á»ng dáº«n thÆ° má»¥c validation: ").strip()
    
    if not os.path.exists(train_dir):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c: {train_dir}")
        return
    
    if not os.path.exists(val_dir):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c: {val_dir}")
        return
    
    use_transfer = input("\nğŸ”„ Sá»­ dá»¥ng Transfer Learning? (y/n): ").strip().lower() == 'y'
    
    epochs_input = input(f"â±ï¸  Sá»‘ epochs (máº·c Ä‘á»‹nh {MODEL_CONFIG['epochs']}): ").strip()
    epochs = int(epochs_input) if epochs_input else MODEL_CONFIG['epochs']
    
    # Training
    try:
        model, history = train_model(
            train_dir, 
            val_dir, 
            epochs=epochs,
            use_transfer_learning=use_transfer
        )
        
        # Váº½ biá»ƒu Ä‘á»“
        plot_training_history(history)
        
        print("\n" + "="*70)
        print("âœ… HUáº¤N LUYá»†N HOÃ€N Táº¤T!")
        print("="*70)
        print(f"Model cuá»‘i cÃ¹ng: {PATHS['model_save']}")
        print(f"Model tá»‘t nháº¥t: {PATHS['best_model']}")
        print(f"Biá»ƒu Ä‘á»“ training: {PATHS['training_plot']}")
        
    except Exception as e:
        print(f"\nâŒ Lá»—i trong quÃ¡ trÃ¬nh training: {str(e)}")


if __name__ == "__main__":
    main()